# neurocti
An LLM for CTI reports: fine-tuning LLMs for working with CTI reports

# neurocti-small

We fine-tuned different open models on top of the [ORKL](https://www.orkl.eu) dataset.

* The [neurocti-small](../neurocti-small) model is built on top of llama-3.1.
* The [neurocti-mistral-nemo-12b](../neurocti-mistral-nemo-12b) model was built on top of [Mistral Nemo](https://mistral.ai/news/mistral-nemo/)

# How to train the models yourself

We aim at replicability and you being able to train your own models as much as possible.
Please see the [training.md](training.md) HOWTO.

# How to run them


# Past presentations
* [Annual FIRST conference](https://www.first.org/conference/2024/program#pNeuroCTI-a-Custom-Fine-Tuned-LLM-for-CTI-Benchmarking-Successes-and-Lessons-Learned) Fukuoka, Japan 2024: [slides](https://www.first.org/resources/papers/conf2024/1115-Neurocti-Kaplan-Dulaunoy-Brandl.pdf)
* Hack.lu 2024: [video](https://youtu.be/sVlLjaTY1pc?feature=shared)
* CERT-Stammtisch 2024: [slides]()
* BSides 2024 Vienna: [slides]()
